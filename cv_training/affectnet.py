import os
import sys
import glob
from tqdm import tqdm
import argparse

from PIL import Image
import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.utils.data as data
from torchvision import transforms, datasets

from networks.dan import DAN


eps = sys.float_info.epsilon
IMG_SIZE = (224, 224)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--aff_path",
        type=str,
        default="/home/and/projects/hacks/hacks_ai_ural_emotion_recognition/emotions-training/experiments/DAN/datasets/young_data",
        help="AfectNet dataset path.",
    )
    parser.add_argument("--batch_size", type=int, default=256, help="Batch size.")
    parser.add_argument(
        "--lr", type=float, default=0.0001, help="Initial learning rate for adam."
    )
    parser.add_argument(
        "--workers", default=4, type=int, help="Number of data loading workers."
    )
    parser.add_argument("--epochs", type=int, default=40, help="Total training epochs.")
    parser.add_argument(
        "--num_head", type=int, default=4, help="Number of attention head."
    )
    parser.add_argument("--num_class", type=int, default=7, help="Number of class.")

    return parser.parse_args()


# class AffectNet(data.Dataset):
#     def __init__(self, aff_path, phase, use_cache=True, transform=None):
#         self.phase = phase
#         self.transform = transform
#         self.aff_path = aff_path

#         if use_cache:
#             cache_path = os.path.join(aff_path, "affectnet.csv")
#             if os.path.exists(cache_path):
#                 df = pd.read_csv(cache_path)
#             else:
#                 df = self.get_df()
#                 df.to_csv(cache_path)
#         else:
#             df = self.get_df()

#         self.data = df[df["phase"] == phase]

#         self.file_paths = self.data.loc[:, "img_path"].values
#         self.label = self.data.loc[:, "label"].values
#         # print(self.label)

#         _, self.sample_counts = np.unique(self.label, return_counts=True)
#         # print(f' distribution of {phase} samples: {self.sample_counts}')

#     def get_df(self):
#         train_path = os.path.join(self.aff_path, "train/")
#         val_path = os.path.join(self.aff_path, "val/")
#         data = []

#         for anno in glob.glob(train_path + "annotations/*_exp.npy"):
#             idx = os.path.basename(anno).split("_")[0]
#             img_path = os.path.join(train_path, f"images/{idx}.jpg")
#             label = int(np.load(anno))
#             data.append(["train", img_path, label])

#         for anno in glob.glob(val_path + "annotations/*_exp.npy"):
#             idx = os.path.basename(anno).split("_")[0]
#             img_path = os.path.join(val_path, f"images/{idx}.jpg")
#             label = int(np.load(anno))
#             data.append(["val", img_path, label])

#         return pd.DataFrame(data=data, columns=["phase", "img_path", "label"])

#     def __len__(self):
#         return len(self.file_paths)

#     def __getitem__(self, idx):
#         path = self.file_paths[idx]
#         image = Image.open(path).convert("RGB")
#         label = self.label[idx]

#         if self.transform is not None:
#             image = self.transform(image)

#         return image, label


class AffinityLoss(nn.Module):
    def __init__(self, device, num_class=8, feat_dim=512):
        super(AffinityLoss, self).__init__()
        self.num_class = num_class
        self.feat_dim = feat_dim
        self.gap = nn.AdaptiveAvgPool2d(1)
        self.device = device

        self.centers = nn.Parameter(
            torch.randn(self.num_class, self.feat_dim).to(device)
        )

    def forward(self, x, labels):
        x = self.gap(x).view(x.size(0), -1)

        batch_size = x.size(0)
        distmat = (
            torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_class)
            + torch.pow(self.centers, 2)
            .sum(dim=1, keepdim=True)
            .expand(self.num_class, batch_size)
            .t()
        )
        distmat.addmm_(x, self.centers.t(), beta=1, alpha=-2)

        classes = torch.arange(self.num_class).long().to(self.device)
        labels = labels.unsqueeze(1).expand(batch_size, self.num_class)
        mask = labels.eq(classes.expand(batch_size, self.num_class))

        dist = distmat * mask.float()
        dist = dist / self.centers.var(dim=0).sum()

        loss = dist.clamp(min=1e-12, max=1e12).sum() / batch_size

        return loss


class PartitionLoss(nn.Module):
    def __init__(
        self,
    ):
        super(PartitionLoss, self).__init__()

    def forward(self, x):
        num_head = x.size(1)

        if num_head > 1:
            var = x.var(dim=1).mean()
            ## add eps to avoid empty var case
            loss = torch.log(1 + num_head / (var + eps))
        else:
            loss = 0

        return loss


class ImbalancedDatasetSampler(data.sampler.Sampler):
    def __init__(self, dataset, indices: list = None, num_samples: int = None):
        self.indices = list(range(len(dataset))) if indices is None else indices
        self.num_samples = len(self.indices) if num_samples is None else num_samples

        df = pd.DataFrame()
        df["label"] = self._get_labels(dataset)
        df.index = self.indices
        df = df.sort_index()

        label_to_count = df["label"].value_counts()

        weights = 1.0 / label_to_count[df["label"]]

        self.weights = torch.DoubleTensor(weights.to_list())

        # self.weights = self.weights.clamp(min=1e-5)

    def _get_labels(self, dataset):
        if isinstance(dataset, datasets.ImageFolder):
            return [x[1] for x in dataset.imgs]
        elif isinstance(dataset, torch.utils.data.Subset):
            return [dataset.dataset.imgs[i][1] for i in dataset.indices]
        else:
            raise NotImplementedError

    def __iter__(self):
        return (
            self.indices[i]
            for i in torch.multinomial(self.weights, self.num_samples, replacement=True)
        )

    def __len__(self):
        return self.num_samples


class AffectNetData(datasets.ImageFolder):
    def find_classes(self, directory: str):
        classes = [
            "neutral",
            "happy",
            "sad",
            "surprise",
            "fear",
            "disgust",
            "anger",
        ]

        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}

        print(classes, class_to_idx)
        return classes, class_to_idx


def run_training():
    args = parse_args()

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.enabled = True

    model = DAN(num_class=args.num_class, num_head=args.num_head)
    checkpoint = torch.load(
        "./checkpoints/affecnet7_epoch6_acc0.6569.pth",
        map_location=device,
    )
    model.load_state_dict(checkpoint["model_state_dict"], strict=True)
    model.to(device)

    data_transforms = transforms.Compose(
        [
            transforms.Resize(IMG_SIZE),
            transforms.RandomHorizontalFlip(),
            transforms.RandomApply(
                [
                    transforms.RandomAffine(20, scale=(0.8, 1), translate=(0.2, 0.2)),
                ],
                p=0.7,
            ),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            transforms.RandomErasing(),
        ]
    )

    train_dataset = AffectNetData(
        f"{args.aff_path}/train",
        transform=data_transforms,
    )

    # train_dataset = AffectNet(args.aff_path, "train", transform=data_transforms)

    if args.num_class == 7:  # ignore the 8-th class
        idx = [i for i in range(len(train_dataset)) if train_dataset.imgs[i][1] != 7]
        train_dataset = data.Subset(train_dataset, idx)

    print("Whole train set size:", train_dataset.__len__())
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        num_workers=args.workers,
        sampler=ImbalancedDatasetSampler(train_dataset),
        shuffle=False,
        pin_memory=True,
    )

    data_transforms_val = transforms.Compose(
        [
            transforms.Resize(IMG_SIZE),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ]
    )

    # val_dataset = AffectNet(
    #     args.aff_path, phase="val", transform=data_transforms_val
    # )  # loading dynamically

    val_dataset = AffectNetData(
        f"{args.aff_path}/val", transform=data_transforms_val
    )  # loading statically
    if args.num_class == 7:  # ignore the 8-th class
        idx = [i for i in range(len(val_dataset)) if val_dataset.imgs[i][1] != 7]
        val_dataset = data.Subset(val_dataset, idx)

    print("Validation set size:", val_dataset.__len__())

    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        num_workers=args.workers,
        shuffle=False,
        pin_memory=True,
    )

    criterion_cls = torch.nn.CrossEntropyLoss().to(device)
    criterion_af = AffinityLoss(device, num_class=args.num_class)
    criterion_pt = PartitionLoss()

    params = list(model.parameters()) + list(criterion_af.parameters())
    optimizer = torch.optim.Adam(params, args.lr, weight_decay=0)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.6)

    best_acc = 0
    best = 0
    for epoch in tqdm(range(1, args.epochs + 1)):
        running_loss = 0.0
        correct_sum = 0
        iter_cnt = 0
        model.train()

        for (imgs, targets) in train_loader:
            iter_cnt += 1
            optimizer.zero_grad()

            imgs = imgs.to(device)
            targets = targets.to(device)

            out, feat, heads = model(imgs)

            loss = (
                criterion_cls(out, targets)
                + criterion_af(feat, targets)
                + criterion_pt(heads)
            )

            loss.backward()
            optimizer.step()

            running_loss += loss
            _, predicts = torch.max(out, 1)
            correct_num = torch.eq(predicts, targets).sum()
            correct_sum += correct_num

        acc = correct_sum.float() / float(train_dataset.__len__())
        running_loss = running_loss / iter_cnt
        tqdm.write(
            "[Epoch %d] Training accuracy: %.4f. Loss: %.3f. LR %.6f"
            % (epoch, acc, running_loss, optimizer.param_groups[0]["lr"])
        )

        with torch.no_grad():
            running_loss = 0.0
            iter_cnt = 0
            bingo_cnt = 0
            sample_cnt = 0
            model.eval()
            for imgs, targets in val_loader:

                imgs = imgs.to(device)
                targets = targets.to(device)
                out, feat, heads = model(imgs)

                loss = (
                    criterion_cls(out, targets)
                    + criterion_af(feat, targets)
                    + criterion_pt(heads)
                )

                running_loss += loss
                iter_cnt += 1
                _, predicts = torch.max(out, 1)
                correct_num = torch.eq(predicts, targets)
                bingo_cnt += correct_num.sum().cpu()
                sample_cnt += out.size(0)

            running_loss = running_loss / iter_cnt
            scheduler.step()

            acc = bingo_cnt.float() / float(sample_cnt)
            acc = np.around(acc.numpy(), 4)
            best_acc = max(acc, best_acc)
            tqdm.write(
                "[Epoch %d] Validation accuracy:%.4f. Loss:%.3f"
                % (epoch, acc, running_loss)
            )
            tqdm.write("best_acc:" + str(best_acc))

            if best < acc:
                torch.save(
                    {
                        "iter": epoch,
                        "model_state_dict": model.state_dict(),
                        "optimizer_state_dict": optimizer.state_dict(),
                    },
                    os.path.join(
                        "checkpoints",
                        "affecnet_res18_finetune_fer_pseudo_young"
                        + "_acc"
                        + str(acc)
                        + ".pth",
                    ),
                )
                best = acc
                tqdm.write("Model saved.")

if __name__ == "__main__":
    run_training()
